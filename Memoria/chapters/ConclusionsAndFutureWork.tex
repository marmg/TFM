%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CONCLUSIONES Y TRABAJO FUTURO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusions and Future Work}
\fancyhead[RE]{\textsc{Chapter} \thechapter. Conclusions and Future Work}
\label{ch:ConclusionsAndFutureWork}
\section{Conclusions}
\label{sec:Conclusions}
\noindent To explain \emph{black box} models such as deep learning models it is a really complex task that, despite of the efforts of the researchers, is still a challenge far away from being completely solved. 
\paragraph{}
Among the last years researchers have developed some interesting approaches that are really helpful to understand the model behavior and to know a bit more about why the model has given an output instead of another one. These approaches are based on some different techniques that help the user to understand what is going on inside the model when giving an output, and to make it easier for the user are often presented with visualization tools and techniques that make really simple to understand the explanations. 
\paragraph{LIME} After several experiments, it can be seen that \emph{LIME} is able to detect the most important words (or spans) the model focus on when making its prediction. But it's still not clear why the model makes a prediction, because \emph{LIME} explains the most important words for each class, but does not explain why the model choose a class over the other ones.
\paragraph{}
This way, \emph{LIME} can help to understand the words that are important for the prediction, an this can help to understand the best way to create questions or to give the model the correct options. But it have been also seen that to change this words or this options does not mean the model is going to change its prediction. For instance, in the experiments in which the most important word ``discipline'' was replaced with a synonym ``orderliness'', the model predicted the good label meanwhile with the word ``discipline'' that appeared in the original text the model did not, and in the experiment in which the options given to the model were perturbed trying to preserve all the important words in the original question (like ``supposed'') the model also failed in its prediction.
\paragraph{}
So it is not clear which one is the correct way to make a question, or why the model has chosen a class instead of another one.
Besides this, when the model fails, \emph{LIME} does not really help to know why the model has failed, because the explanations of each of the classes will give us the important words for that class, but will not tell why a class is chosen beyond the others. 
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/conclussionLime}
	\caption{Explanation of all options given by \emph{LIME}}
	\label{fig:conclussionLime}
\end{figure}
\paragraph{BertVIZ} \emph{BertViz} is a tool to explore and visualize \emph{attention} weights in every \emph{transformer-based} model. It allows the user to visualize \emph{attention} weights in an easy and user-friendly way, making possible to see and detect bias and patterns in the \emph{attention} mechanism as well as to detect useless heads to be able to prune them and save space and time without loosing performance.
\paragraph{}
But, \emph{BertViz} is just that, a visualization tool. It does not provide or perform any additional operation over the model and, therefore, it does not help more than other visualization tools to understand what is going on inside the model. Although, as said before, it helps to visualize \emph{attention} weights to, for instance, detect patterns or see how words relate with others, this is not helpful when trying to understang the model behaviour and to know why the model has given an output and why not another one. But \emph{BertViz} is not guilty for this, because as some research articles\cite{} has pointed out recently, to visualize the \emph{attention} weights does not really provide an explanation of the local prediction, because the \emph{attention} mechanism is just used to generate contextual embeddings of the input. Instead of that it would be more useful to explain the final layers of the model, that are the ones that are really solving the task, but this is very difficult, as they have as input the embeddings generated by the full model, making it really difficult to explain the layers behavior and to understand the explanations.
\paragraph{}
Therefore, other techniques such as \emph{LIME} that create a surrogate explainable model are more useful than to visualize the \emph{attention} weights in order to explain the model behavior.
\paragraph{GPT-3}
Despite of having some good explanations from \emph{GPT-3} explaining itself, it is not clear if \emph{GPT-3} is able to explain itself. When the explanation is in the article or it's easy to explain, it has not problem to do it. But when the question or the explanation are more difficult, not always give a valid explanation (sometimes nor even a good answer).
\paragraph{}
Although the explanations are not always valid, they are gramatically correct, taking sentences from the article. But sometimes they are not even related to the question, other times it gives the correct explanation, as in \ref{fig:gpt3-result-correct}, where the option was \emph{America} and the explanation is: \emph{It says that he came from New York}, which is indeed in America but the lack of explanation relating New York to America could make it a bad explanation that is not enough for a user to understand the result, as seen in \ref{sec:qa-evaluation}, the completeness was one of the \emph{criteria} used by \cite{Allam2012} to select an answer as correct. Did the model relate New York with America to choose it as the result and the explanation? or did it just focus on that the question was asking for a place? This is not clear as the explanation was not consistent and not complete. So, this explanation, as seen in \ref{sec:qa-evaluation} and following the classification of \cite{Pablo-Sanchez}, this answer could be classified as incomplete. But this classification was thought for answers of \emph{QA} systems, not for explanations. As seen in \ref{sec:xai-evaluation} there is not a common way to evaluate the explanations what can lead to a lack of quality in the results. For instance in this case, in which \emph{GPT-3} is giving an explanation in natural language, it is easier to understand for the user, but the explanation is not consistent nor complete, so the answers classification could be used for this type of explanations.
\section{FutureWork}
\label{sec:FutureWork} 