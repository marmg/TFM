%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% EVALUACION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results}
\fancyhead[RE]{\textsc{Chapter} \thechapter. Results}
\label{ch:Results}
\noindent Before going deeper into the results, the experiments and examples used to test the tools are being introduced. As tools are different depending to its nature (model agnostic vs model specific), different experiments have to be done.
\paragraph{}
Besides this, examples used are going to be the same to be able to compare the results. 3 examples of the \emph{RACE test set} have been selected and modified to test the tools. The original examples selected and the modifications are listed below. The part of the article where the answer is appears in bold.
\paragraph{Example 1}
\begin{passage}[Article of Example 1]{art:1}
Take a class at Dulangkou School, and you'll see lots of things different from other schools, You can see the desks are not in rows and students sit in groups. They put their desks together so they're facing each other. How can they see the blackboard? There are three blackboards on the three walls of the classroom! \\
The school calls the new way of learning ``Tuantuanzuo'', meaning sitting in groups. Wei Liying, a Junior 3 teacher, said it was to give students more chances to communicate. \\
Each group has five or six students, according to Wei, and they play different roles .There is a team leader who takes care of the whole group. There is a ``study leader'' who makes sure that everyone finishes their homework. \textbf{And there is a discipline leader who makes sure that nobody chats in class}. \\
Wang Lin is a team leader. The 15-year-old said that having to deal with so many things was tiring. \\
``I just looked after my own business before,''said Wang. ``But now I have to think about my five group members.'' \\
But Wang has got used to it and can see the benefits now. \\
``I used to speak too little. But being a team leader means you have to talk a lot. You could even call me an excellent speaker today.'' \\
Zhang Qi, 16, was weak in English. She used to get about 70 in English tests. But in a recent test, Zhang got a grade of more than 80.\\
``I rarely  asked others when I had problems with my English tests. But now I can ask the team leader or study leader. They are really helpful.''
\end{passage}
Question: ``\emph{A discipline leader is supposed to  \_  .}''\\
Options: 
\begin{itemize}
 \item A: take care of the whole group.
 \item B: make sure that everybody finishes work
 \item C: make sure that nobody chats in class
 \item D: collect all the homework and hand it in to teachers
\end{itemize}
Correct Answer: C. \\ 
Prediction: C. \\
\textbf{$\bullet$ Modification $a$:} \\
Description of the modification: Question paraphrased.\\
New question: \emph{What is a discipline leader supposed to?} \\
Prediction: A. \\
\textbf{$\bullet$ Modification $b$:} \\
Description of the modification: Question and options paraphrased.\\
New question: \emph{What is a discipline leader?} \\
New options:
\begin{itemize}
 \item A: A person supposed to take care of the whole group
 \item B: A person supposed to make sure that everybody finishes work
 \item C: A person supposed to make sure that nobody chats in class
 \item D: A person supposed to collect all the homework and hand it in to teachers
\end{itemize}
Prediction: B. \\
\textbf{$\bullet$ Modification $c$:} \\
Description of the modification: Question paraphrased and changed with a synonym and options paraphrased.\\
New question: \emph{What is an orderliness leader?} \\
New options:
\begin{itemize}
 \item A: A person supposed to take care of the whole group
 \item B: A person supposed to make sure that everybody finishes work
 \item C: A person supposed to make sure that nobody chats in class
 \item D: A person supposed to collect all the homework and hand it in to teachers
\end{itemize}
Prediction: C.
\paragraph{Example 2}
\begin{passage}
A traveler came out of the airport. There were a lot of taxis. He asked every taxi driver about his name. Then he took the third one. It cost 5 dollars from the airport to the hotel. ``How much does it cost for the whole day?'' The man asked. ``100 dollars,'' said the taxi driver. This was very dear, but the man said it was OK. \\
The taxi driver took the man everywhere. He showed him all the parks and museums in the city. In the evening they went back to the hotel. The traveler gave the taxi driver 100 dollars and said, ``What about tomorrow?'' The taxi driver looked at the man and said, ``Tomorrow is another 100 dollars.'' And the man said, ``That's OK! See you tomorrow.'' The taxi driver was very pleased. \\
The next day the taxi driver took the traveler everywhere again. They visited all the parks and museums again. And in the evening they went back to the hotel. The man gave the taxi driver 100 dollars again and said, ``I'm going home tomorrow.'' The driver was sorry because he liked the traveler and 100 dollars a day was a lot of money. ``So you are going home. Where do you come from?'' He asked. ``I come from New York.'' ``New York,'' the taxi driver said, ``I have a sister in New York. Her name is Susan. Do you know her?'' ``Of course I know her. She gave me 200 dollars for you!'' \\
\end{passage}
Question: ``\emph{Where did the traveler come from?}''\\
Options: 
\begin{itemize}
 \item A: England
 \item B: America
 \item C: Canada
 \item D: France
\end{itemize}
Correct answer: B. \\
Prediction: A\\
\textbf{$\bullet$ Modification $a$:} \\
Description of the modification: Question paraphrased.\\
New question: \emph{The traveler came from \_ .} \\
Prediction: C. \\
\textbf{$\bullet$ Modification $b$:} \\
Description of the modification: Question and options paraphrased.\\
New question: \emph{The traveler \_ .} \\
New options:
\begin{itemize}
 \item A: came from England
 \item B: came from America
 \item C: came from Canada
 \item D: came from France
\end{itemize}
Prediction: C. \\
\textbf{$\bullet$ Modification $c$:} \\
Description of the modification: Question paraphrased and changed with a synonym and options paraphrased.\\
New question: \emph{The visitor \_ .} \\
New options:
\begin{itemize}
 \item A: came from England
 \item B: came from America
 \item C: came from Canada
 \item D: came from France
\end{itemize}
Prediction: C.
\section{Model-agnostic experiments}
\label{sec:ModelAgnosticExperiments}
\noindent Most of the model agnostic tools are based on instance perturbation, to train an underlying model easily interpretable that emulates the \emph{LM} results.
\paragraph{}
Four different experiments have been developed to test the model agnostic tools and to try to understand which parts are more important for the model in order to make its prediction.
\begin{enumerate}
	\item The first one was based on perturb just the question. \\
	The first idea was to try to explain which words of the question are important for the prediction, because at the end the model has to take as input the question and look for the answer. This way, the question the user asks to the model will determine the result of the prediction, and to know which words of the question are important is a good way to know how to make good questions.
	\item In the second one, the options given to the model were perturbed. \\
	After the question, the most important part on the multiple-choice problems is the set of options given to the model, because it has to choose between some of this options.
	\item The third one consisted on perturb the article. \\
	After all, the model has to look for the answer in the article, so it is important to that the model understands it. Thus, in the third experiment the article was perturbed.
	\item In the final one, both the options and the question were perturbed. \\
	As sometimes question and options are related, sometimes even the options are the continuation of the question, it is also important to test the tools perturbing both options and question.
\end{enumerate}
\paragraph{}
The main idea behind these experiments is to check if the tools are able to detect which parts are the most important for the model (the article, the question or the options), and this way know if the tools are useful to understand the result and if the result is legit or not. Besides this, to look for the words that are important for the prediction result is also expected from the experiments. 
\subsection{LIME}
\label{sec:LIMEResults}
\noindent As seen before, \emph{LIME} is a model agnostic tool based on instance perturbation. Thus, the first experiment consisted on make \emph{LIME} perturb just the question and see what words are important for the model. This was tested with different samples of the test set (see \ref{ex:intro}) and the results showed that \emph{LIME} is able to detect the words that are indeed important for the model, as can be seen in Figure \ref{fig:lime-result-q-1a}.
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex1-example1-real}
	\caption{\emph{LIME} result for the first experiment with original example.}
	\label{fig:lime-result-q-1a}
\end{figure}
\paragraph{}
In that example, the real label was $C$, and \emph{LIME} was able to detect that the important word was ``discipline'', so if this word is changed the result will vary, but if the rest of the question is changed, the result shouldn't change too much. 
So the next steps were to check if that idea was true. In order to do this some changes were made in both the question and the options to see if the result was the same.

\paragraph{}
Following with the experiments planned before, the first modification was made in order to see if the question form is important for the model. As the important word, ``discipline'', has not being removed, the model shouldn't have any problem to get the correct answer. Nevertheless, the model failed by selecting the answer $A$  instead of the correct answer in this case. 
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex1-example1-modA-real}
	\caption{\emph{LIME} result for the first experiment with the first change on the question.}
	\label{fig:lime-result-q-1b}
\end{figure}
\paragraph{}
But, attending to the probabilities of the prediction, it can be seen that both $A$ and $C$ options have almost the same probability. And, according to \emph{LIME}, ``discipline'' is still important for the $C$ option.

\paragraph{}
The next change was made to test if the question form is really important for the model. The change was made trying to do not change the meaning of the original ones, but adapting them to the answer expected by a ``What is...'' question.
The result in this case was that the model also failed, but in this case it fails by choosing ``B'' as the correct answer.
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex1-example1-modB-real}
	\caption{\emph{LIME} result for the first experiment with the second change on the question.}
	\label{fig:lime-result-q-1c}
\end{figure}

\paragraph{}
To see if the question form was indeed so important and to see if the ``discipline'' word was important or not, a second change was made. This change consisted on replacing the ``discipline'' word with a synonym, ``orderliness''. This is interesting because in the original text the word ``discipline'' appears with the correct answer, but the word ``orderliness'' don't appear at all, so if the model is able to choose the correct answer could be due to the understanding of the model of the language, knowing the synonyms. 
\paragraph{}
So the changes made before by paraphrasing the question were made now, but replacing ``discipline'' with ``orderliness''. The results were interesting because, in this case, the model chose the correct answer in both cases, something that didn't happened while preserving the original ``most important'' word. 
\paragraph{}
As said before this could be because of the ability of the model to understand the language, but this was just an idea until \emph{LIME} was used to check the importance of the word to see that the word ``orderliness'' was the most important word of the new question. The important thing here is that the importance score of ``orderliness'' is bigger in this case than the importance score of ``discipline'' in the other questions, what could explain the fact that in this case the model has been able to find the correct answer.
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex1-example1-modC-real}
	\caption{\emph{LIME} result for the first experiment with the third change on the question.}
	\label{fig:lime-result-q-1d}
\end{figure}

\paragraph{}
 Taking this into account, it is different the way to understand the model if it is a multiple-choice problem or an extractive question answering problem.
\paragraph{}
When analyzing the explanations obtained with \emph{LIME} for the same test example, it has been seen that \emph{LIME} is able to detect the words of the label being explained, but it is curious that an \emph{a priori} important word like ``chats'' it is not weighted by \emph{LIME}, nor positive nor negative, with all experiments and modifications done.
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex2-example1-real}
	\caption{\emph{LIME} result for the second experiment.}
	\label{fig:lime-result-2a}
\end{figure}
This is curious, because the other words of the options are common and/or related to the same domain, the school, meanwhile ``chats'' is a verb of a different domain -and more related to ``discipline'', the most important word of the question, but ``nobody'', which is also a ``different'' word and related in someway to ``discipline'' it is detected and it's detected as the most important word for the model, which makes sense.

\paragraph{}
In the third experiment the article was perturbed to see if \emph{LIME} was able to find the most important words within it. The results showed that \emph{LIME} knows how to deal with large texts (inputs), but as the model's input size is not too large (due to hardware limitations in the training phase), the most important words of the text can not be captured by \emph{LIME}.
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex3-example1-real}
	\caption{\emph{LIME} result for the third experiment.}
	\label{fig:lime-result-3a}
\end{figure}
\paragraph{}
Even when this is obviously a problem, the experiment is not a complete failure, because it has been seen one of the most possible causes for the model to fail, this is, the input size is not as large as it should. For this kind of problems in which the input are large texts (such articles), the input size plays an important role in the results.
\paragraph{}
Nevertheless the model is still able to choose the correct option sometimes. This could be due to the knowledge the model has about the language. In these cases, where the model can't see the sentences the evidence is into, the model knows which one is the correct answer by understanding what the question requires and the meaning of the important words of the question and linking them to the most similar answer.
\paragraph{}
In the final experiment, both the question and the options are going to be analyzed. The results showed that the model focus more on the options than in the question. Even when explaining a specific label, \emph{LIME} returns a greater weight for words in other options than in the question itself.
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{images/lime-ex4-example1-real}
	\caption{\emph{LIME} result for the fourth experiment.}
	\label{fig:lime-result-4a}
\end{figure}
This could mean that, if that words of other options are changed, the model could choose them as the correct answer.

\section{Model-specific experiments}
\label{sec:ModelSpecificExperiments}
\noindent 
\subsection{Attention Heatmap}
\label{sec:AttentionHeatmapResults}
\noindent As said before, to visualize the \emph{Attention} values as a heatmap is a common method (first introduced in the original \emph{Attention} paper, \cite{Bahdanau2014}. In this case a simple heatmap has been plotted to see how \emph{tokens} at the question attend to \emph{tokens} at each of the options. As the \emph{attention} heatmap takes the \emph{attention} values of each of the heads, it has been decided to make the sum of the values of each head to get a unique value of each \emph{token} relationship. This way, the heatmaps of figures \ref{fig:attention-heatmap-c} and \ref{fig:attention-heatmap-examples} shows the sum of the \emph{attention} values of each head of \emph{tokens} in x-axis attending to \emph{tokens} in y-axis.
\paragraph{}
As can be seen in Fig \ref{fig:attention-heatmap-c}, the \emph{discipline token} that is the most important one (according to other methods and having into account that is the \emph{token} that differs with other types of leaders in the text) has not a special value or relationships with other important \emph{tokens} in the sentence, such as \emph{nobody chats in class}. 
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{images/attention_heatmap_2}
	\caption{\emph{Attention} heatmap for correct answer.}
	\label{fig:attention-heatmap-c}
\end{figure}
\paragraph{}
However, when looking at the heatmaps of other options, Fig \ref{fig:attention-heatmap-examples}, it can be seen that the \emph{discipline token} has higher values when attending to \emph{homework}, something that can have learn during the training, as someone with discipline is someone that always does the homework. Also when looking at the whole picture (heatmap of each option), it can be seen that there is no a special pattern or something that helps to understand the model decision. 
\begin{figure}[h]
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
	\includegraphics[width=200px]{images/attention_heatmap_0}
	\caption{\emph{Attention} heatmap option A.}
	\label{fig:att-hm-a}
\end{subfigure}
% \medskip\\
\begin{subfigure}{0.45\textwidth}
  \centering
	\includegraphics[width=200px]{images/attention_heatmap_1}
	\caption{\emph{Attention} heatmap option B.}
	\label{fig:att-hm-b}
\end{subfigure}
\medskip\\
\begin{subfigure}{0.45\textwidth}
  \centering
	\includegraphics[width=200px]{images/attention_heatmap_2}
	\caption{\emph{Attention} heatmap option C.}
	\label{fig:att-hm-c}
\end{subfigure}
% \medskip\\
\begin{subfigure}{0.45\textwidth}
  \centering
	\includegraphics[width=200px]{images/attention_heatmap_3}
	\caption{\emph{Attention} heatmap option D.}
	\label{fig:att-hm-d}
\end{subfigure}
\caption{\emph{Attention} Heatmaps.}
\label{fig:attention-heatmap-examples}
\end{figure}


\subsection{BertViz}
\label{sec:BertVizResults}
\noindent Due to the problems of \emph{BertViz} to deal with large inputs, only has been tested the relations regarding the question and a single answer. This is, however, enough to test the tool, because it works in the same way no matter the input and, as it does not have to process anything, it does not matter the input used to test the tool and see how it works and how much information provides.
\paragraph{}
The first and most common way to study the \emph{Attention} layers is to use the \emph{Head View}, in which \emph{BertViz} shows the \emph{Attention} weights of a single layer. Again, the example used to test the tool has been: \textit{A discipline leader is supposed to  \_  .} 
\paragraph{}
When looking at how \emph{tokens} in Question attend to \emph{tokens} in each Answer, it can be seen that there is not anything that clarifies why the model chooses an option among the other. Figure \ref{fig:bertviz-q-options} shows layer 0 for each option. It can be seen that, for instance, \emph{discipline} attends more to \emph{tokens} such as \emph{class} or \emph{homework}, what makes sense. 
\begin{figure}[h]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
	\includegraphics[width=120px]{images/bertviz-question-a}
	\caption{Question attending to Answer A.}
	\label{fig:bertviz-q-aa}
\end{subfigure}
\medskip
\begin{subfigure}{0.4\textwidth}
  \centering
	\includegraphics[width=120px]{images/bertviz-question-b}
	\caption{Question attending to Answer B.}
	\label{fig:bertviz-q-ab}
\end{subfigure}
\medskip\\
\begin{subfigure}{0.4\textwidth}
  \centering
	\includegraphics[width=120px]{images/bertviz-question-c}
	\caption{Question attending to Answer C.}
	\label{fig:bertviz-q-ac}
\end{subfigure}
\medskip
\begin{subfigure}{0.4\textwidth}
  \centering
	\includegraphics[width=120px]{images/bertviz-question-d}
	\caption{Question attending to Answer D.}
	\label{fig:bertviz-q-ad}
\end{subfigure}
\caption{\emph{BertViz} Head View of Question to Answers.}
\label{fig:bertviz-q-options}
\end{figure}
\paragraph{}
For instance, it can be seen in Figure \ref{fig:bertviz-l11h2} that \emph{discipline} attends to \emph{make sure that nobody}, which is really related to \emph{discipline}. However, there are different patterns across the layers and heads, and each head in each layer attend to \emph{tokens} by following a specific pattern. 
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.4]{images/bertviz-question-l11-head2}
	\caption{\emph{BertViz} layer 11, head 2.}
	\label{fig:bertviz-l11h2}
\end{figure}
\paragraph{}
As there are usually too many heads (model used has 12 heads per each one of the 12 layers), it costs a lot to look at each one separately. In order to take a global look and to discover patterns and useless heads, the \emph{BertViz Model View} is used. In Figure \ref{fig:bertviz-model-q} it can be seen that the \emph{Attention} layers follow patterns, even when the option change.
\begin{figure}[!h]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
	\includegraphics[width=140px]{images/bertviz-model-a}
	\caption{Question attending to Answer A.}
	\label{fig:bertviz-model-q-aa}
\end{subfigure}
\medskip 
\begin{subfigure}{0.4\textwidth}
  \centering
	\includegraphics[width=140px]{images/bertviz-model-b}
	\caption{Question attending to Answer B.}
	\label{fig:bertviz-model-q-ab}
\end{subfigure}
\caption{\emph{BertViz} Model View of Question to Answers A and B.}
\label{fig:bertviz-model-q}
\end{figure}
\paragraph{}
Once that a pattern have been detected (or an outlier) it's useful to look at that specific head. The tool let the user to do this by clicking in the corresponding head, Figure \ref{fig:bertviz-model-head}. This way the user can see what \emph{tokens} are being attended at in the patterns, e.g. nouns, prepositions, etc. 
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.4]{images/bertviz-model-detail}
	\caption{To look at a single head in the Model View of \emph{BertViz}.}
	\label{fig:bertviz-model-head}
\end{figure}
\paragraph{}
The model view is useful to have a global look of the model, to see its behaviour and to see what heads are useful/useless. For instance, it can be seen that a lot of heads follow the same pattern, all \emph{tokens} attend to the \emph{[SEP] token}, what is considered as a \emph{null head}. These heads, therefore, could be prune to save space and time. This could be due to the training of the model, meaning that the model could be underfitted and, therefore, should be trained with more data or during more time, which in this case is true, because due to hardware and temporal limitations the model has been training for just 1 day, which is not enough for a \emph{LM}.
\paragraph{}
Therefore, to look at the \emph{Attention} does not really provide an explanation of why the model choose one option, because it follows patterns and does not change over the different options. 
\paragraph{}
Although \emph{BertViz} does not help to explain a prediction of the model, it is a good tool for visualizing the \emph{Attention} weights, what can be useful as said before to, for instance, prune useless heads. But it's not a good option for explain a prediction of the model.

\cleardoublepage
\subsection{Intrinsic GPT-3}
\label{sec:GPT3Results}
\noindent In this case, three passages were selected from the test set to feed \emph{GPT-3} and force it to generate the text as wished, this is, an explanation of the answer. These passages were complex to answer, because in some of them the answer was not in the text itself, but related to. For instance, in one of the passages the question asks about the location of the story, given as options: England; America; Japan; Australia. But in the text it talks about New York, so a bit of commonsense is needed in order to relate New York with America.
\paragraph{}
The examples feed to the model included the context, the question, the options, the correct answer and a simple explanation of why it is the correct answer. The appendix \ref{ch:GPT3-Examples} includes all the examples feed to the model. 
\paragraph{Example 1}
The first try was using the Example 1, \ref{ex:1}. As the answer was in the text it was not surprising that \emph{GPT-3} predicted it correctly. Regarding the explanation, Figure \ref{fig:gpt3-ex1}, it is a good explanation but, again, as the answer is inside the text there was not too much to explain but say that the article says so.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{images/gpt3-ex1}
	\caption{Explanation of GPT3 for the Example 1.}
	\label{fig:gpt3-ex1}
\end{figure}
\paragraph{Example 2}

\paragraph{}
Surprisingly, the result of \emph{GPT-3} included the correct answer and an accurately explanation, \ref{fig:gpt3-result-correct}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{images/gpt3-correct}
	\caption{Explanation of GPT3.}
	\label{fig:gpt3-result-correct}
\end{figure}
\paragraph{}
But it has been detected that it is not always that good, and therefore it is not clear if it's indeed able to explain itself. With a simple change of options, interchanging the options A and B, the result fails in the prediction, the explanation is the same though, \ref{fig:gpt3-result-fail1}. And if an extra ``answer'' token is added to the end, the result of the prediction is correct but it fails in the explanation, \ref{fig:gpt3-result-fail2}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{images/gpt3-fail1}
	\caption{Wrong prediction of GPT3.}
	\label{fig:gpt3-result-fail1}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{images/gpt3-fail2}
	\caption{Wrong explanation of GPT3.}
	\label{fig:gpt3-result-fail2}
\end{figure}