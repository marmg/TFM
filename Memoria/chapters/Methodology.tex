\chapter{Methodology}
\fancyhead[RE]{\textsc{Chapter} \thechapter. Methodology}
\label{ch:Methodology}
\noindent The proposal of this work is to review the state of the art of \emph{XAI} of \emph{LM} applied to multiple choice problems. In this chapter the methodology followed is explained.
\section{Description}
\label{sec:Description}
\noindent In order to test the different \emph{XAI} tools, a language model for a multiple choice problem is needed. So the first step covered is to train a \emph{LM} to be used. Then the different \emph{XAI} tools to be tested are going to be adapted to multiple choice problems.
\section{Model Training}
\label{sec:Model training}
\noindent As said before, a language model was trained for a multiple choice problem. Although it is not the best choice for a real problem (nor the one that has the best performance nor the fastest model), \emph{BERT} has been chosen, because it is the most famous language model. 
\paragraph{}
Therefore, a \emph{BERT} model is going to be trained over one of the multiple choice \emph{datasets} seen in Section 2.2.2. The \emph{dataset} selected was \emph{RACE}, taken from the Huggingface \emph{datasets} library. \footnote{\url{https://huggingface.co/docs/datasets/}}. 
\paragraph{}
The implementation of \emph{BERT} used was the one available at the Huggingface \emph{transformers} library. \footnote{\url{https://huggingface.co/transformers/}} Due to hardware limitations the \emph{BERT base} version was used, and as it is not as powerful as the largest version, the \emph{RACE-middle} version of the \emph{dataset} was used. The accuracy obtained after 3 epochs was of $0.641$. Although it could be improved, the goal of this work is not to train a good model for the multiple choice problem but to explain the model decisions, both right and wrong. The code to train the model can be found in:
\begin{itemize}
	\item GitHub: \url{https://github.com/marmg/TFM/blob/main/BERT-RACE.ipynb}
	\item Google Collab: \url{https://colab.research.google.com/drive/1RexG8T8Rz8QvVkX0H-K5VaXpuSY-mAEJ?usp=sharing}
\end{itemize}

\subsection{Explainability model-agnostic}
\label{sec:ModelAgnostic}
\noindent As seen in \ref{sec:xai-classification}, there are different types of \emph{XAI} tools. In this work both model-agnostic and model-specific tools are going to be tested. The probably most famous \emph{XAI} tool is \emph{LIME}, and therefore is going to be tested.
\subsubsection{LIME}
\label{sec:Lime}
\noindent In order to use \emph{LIME} with \emph{BERT} for Multiple-Choice (the same as with any other model), a function has to be developed that takes as input a list of perturbed instances, makes the prediction and return the probability for each class. 
\paragraph{}
Even when is not the efficient, the easiest and most common way to fit the model is by repeating the article, joined to the question and one of the options, for instance while predicting in \emph{RACE} dataset, the result would be four instances with the same article and the same question, but with a different option each. 
\paragraph{}
So, for developing the function used by \emph{LIME}, this four instances have to be created inside it with the perturbations made by the method. Depending on the experiment, this instances will be different because the perturbations will be just the question, the article, the options and so on, and will be concatenated with the original parts that are not being perturbed.
\paragraph{}
Then, the instances are converted to the features to make the prediction. The result is transformed with a \emph{Softmax} function and finally is returned to \emph{LIME}, which analyzes the outputs related to the perturbations made, given as the result a score based on the importance of the word for the prediction.
\paragraph{}
\emph{LIME} works by perturbing the instance to measure the importance of the words, that is enough for most of the \emph{NLP} tasks such as Text Classification. But in Multiple-Choice task there are different parts to take into account, the article or context, the question and the options. Thus, four experiments are going to be made to explore each of the parts.
\begin{enumerate}
	\item 1st experiment: The question is going to be perturbed.
	\item 2nd experiment: The options are going to be perturbed.
	\item 3rd experiment: The article is going to be perturbed.
	\item 4th experiment: The question and the options are going to be perturbed.
\end{enumerate}
With this, not only the most important words are expected to be detected, but also which part is more important for the model.
\paragraph{}
As \emph{LIME} takes as input the instance to perturb and the prediction function, different functions have to be developed in order to make the experiments. One of them will take as input the question and will generate the features of the example by using the original article and options.Other will take as input the options and once again will generate the features of the example by using the original article and, in this case, the original question. The same with the article and with the question and options together. 
\paragraph{}
When there are more than one instance (like in the options), they are going to be concatenated with a special character and the function will split it to generate the different examples.

\subsection{Explainability model-specific}
\label{sec:ModelSpecific} 
\noindent Among the different model-specific options available to study the \emph{LM}, the most famous and useful is to look at the \emph{Attention} weights. \emph{Attention} is used to vectorize the words generating contextual embeddings, taking into account how words relate themselves and what words are in the text to understand the real meaning of the words.
\paragraph{}
This way, to look at the \emph{Attention} has proved to be useful to understand the model behaviour and to be able to prune heads of the \emph{LM}, reducing the size of the model, the time or the hardware needed and, therefore, several tools have been developed to study and to look at the \emph{Attention} layers.

\subsubsection{Attention Heatmap}
\label{sec:AttentionHeatmap}
\noindent The \emph{attention} heatmap is a correlation plot that show hot \emph{tokens} in one sentence attend to \emph{tokens} in other (or the same) sentence. As this only depends on the \emph{tokens} and values that are shown, it is possible to plot a heatmap of \emph{Multiple Choice} and \emph{QA} problems, showing how \emph{tokens} at question attend to \emph{tokens} at options.

\subsubsection{BertViz}
\label{sec:Bertviz}
\noindent Although \emph{BertViz} does not support \emph{QA} or \emph{Multiple Choice} problems by default, it is possible to apply it to these types of problems by modifying the source code, which is open and available in GitHub\footnote{\url{https://github.com/jessevig/bertviz}}.
\paragraph{}
\emph{BertViz} takes as input the \emph{attention} weights and use them to draw the correlation between \emph{tokens} based on these values. Thus, if the \emph{attention} values of a \emph{Multiple Choice} problem is taken as input and it is parsed it is possible to adapt the tool for these problems. The modifications done format the \emph{attention} values to show how \emph{tokens} at question attend to \emph{tokens} at each of the options and vice versa.

\subsection{Intrinsic GPT-3}
\label{sec:gpt3}
\noindent Following the idea of Rajani et al. \cite{Rajani2019} and trying to take advantage of the power of \emph{GPT-3}, one test was developed in order to see if \emph{GPT-3} was able to explain itself. 
\paragraph{}
To force \emph{GPT-3} to generate the explanations (or any expected result), first a few examples of what is exactly desired have to be feed to the model in order to show it what is expected. In this case the model is going to be feed with some examples following the next structure:
\begin{itemize}
	\item Context
	\item Question
	\item Options: Options formatted with an identifier (A-D).
	\item Answer: Identifier of the option chosen. 
	\item Explanation: Explanation of the answer
\end{itemize}
In Appendix \ref{ch:GPT3-Examples} the examples used are shown.
\paragraph{}
